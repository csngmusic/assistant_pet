Данный проект будет содержать ассистента для студента, 
который будет отвечать на заданные ему вопросы, основываясь на переданной ему литературе.

1. Установить Tesseract OCR, путь к файлу заменить в config.py
2. Установить Poppler для Windows, также заменить путь к файлу
3. Установить Ollama для Windows.
4. Скачать LLM Gemma2:9b (ollama pull gemma2:9b). При установке другой модели обновить config.py
5. Установить зависимости (requirements.txt).
6. Создать хранилище данных (Запросы в файле "database.xql")
7. Прописать файл "config_db.py" (Структура в "config_db_template.py")
8. Запустить сервер через uvicorn.

Для добавления новых файлов запустить функцию "parse_text" из "books.py" (в консольке питона или вызвать в другом файле)
Запуск производится на порте 8080, пример запуска ниже:
uvicorn server:app --host 0.0.0.0 --port 8080 --reload
Запуск произведется в локальной сети на порте 8080. При изменении файлов сервер перезапустится.
При первом открытии страницы запускается LLM, поэтому браузер может пожаловаться на невозможность открыть страницу. Это норма, через некоторое время запустится модель, а после страница. Для душевного спокойствия рекомендуется проверять используемые приложением ресурсы, будет видно, когда модель запустится.
